import asyncio
import json
import os
from urllib.parse import urlencode, urlparse, parse_qs
import pandas as pd
from scrapling.fetchers import StealthyFetcher
import pathlib
import random
from datetime import datetime, timedelta
import logging
import sys
from typing import Dict, List, Optional, Tuple, Any

BASE_FILE = pathlib.Path(__file__).resolve().parent

# File configurations
SOURCE_DATA = input("ğŸ“ Enter path to your input XLSX file: ").strip().strip('"\'')
# SOURCE_DATA = '/Users/seyaul/hana inc projects/simpledimplewebscraper/Scrapling/scraplingAdaptationHana/source_prices.xlsx'
OUTPUT_DATA = BASE_FILE / "giant_price_compare/giant_foods_comparison.xlsx"
CHECKPOINT_FILE = BASE_FILE / "giant_scraping/catalog_checkpoint_giant.json"
DUMP_FILE = BASE_FILE / 'giant_scraping/scraped_products_dump.csv'
LOG_FILE = BASE_FILE / ".giant_log/giant_catalog_scrape.log"
CATEGORY_MAPPING_FILE = BASE_FILE / "giant_scraping" / "giant_category_mapping.json"

# Ensure directories exist
for file_path in [CHECKPOINT_FILE, DUMP_FILE, LOG_FILE, OUTPUT_DATA]:
    file_path.parent.mkdir(parents=True, exist_ok=True)

# Scraping configurations
FACILITY_ID = "50000732"
ZIP_CODE = "20010"
STORE_ADDRESS = "1345 Park Road N.W."

# Main categories to scrape
MAIN_CATEGORIES = {
    "2098": "Produce",
    "1563": "Meat",
    "1633": "Seafood", 
    "921": "Deli & Prepared Food",
    "805": "Dairy & Eggs",
    "85": "Bread & Bakery",
    "365": "Beverages",
    "1066": "Rice, Pasta & Beans",
    "166": "Baking & Cooking",
    "569": "Condiments & Sauces",
    "2708": "Soups & Canned Goods",
    "530": "Breakfast",
    "2543": "Snacks",
    "7099": "Candy & Chocolate",
    "2": "Adult Beverages",
    "1448": "Laundry, Paper & Cleaning",
    "1675": "Home & Office",
    "6477": "Floral & Garden",
    "1201": "Health & Beauty",
    "1370": "Baby",
    "2057": "Pets"
}

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler(sys.stdout),
    ],
)
log = logging.getLogger(__name__)

def normalize_upc_input(upc):
    """Normalize UPC format for better matching"""
    if pd.isna(upc):
        return None
    upc_str = str(upc).strip()
    upc_str = upc_str[:-1]  # Remove check digit
    return upc_str.lstrip('0') if upc_str != '0' else '0'

def normalize_upc_scraped(upc):
    """Normalize UPC format for better matching"""
    if pd.isna(upc):
        return None
    upc_str = str(upc).strip()
    return upc_str.lstrip('0') if upc_str != '0' else '0'

def load_giant_progress():
    """Load previous scraping progress"""
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, 'r') as f:
            data = json.load(f)
    else:
        data = {
        'completed_categories': [], 
        'failed_categories': [],
        'seen_upcs': [],
        'overall_total': 0,
        'category_totals': {cat_id: 0 for cat_id in MAIN_CATEGORIES.keys()},
        'scroll_limit_reached': {limbool : False for limbool in MAIN_CATEGORIES.keys()},
        'all_in_category' : None,
        'seed_payload': None,
    }
    data["seen_upcs"] = set(data.get("seen_upcs", []))  # Ensure seen_upcs is a set
    return data

def load_category_map(path: Optional[pathlib.Path | str] = None) -> Dict[str, Any]:
    """Load the JSON mapping generated by *giantcategorydiscovery.py*.

    The file is usually  *BASE_DIR / giant_scraping / giant_category_mapping.json*  but you
    can override with *path*.
    """
    path = pathlib.Path(path) if path else CATEGORY_MAPPING_FILE
    if not path.exists():
        print(f"âš ï¸  Categoryâ€‘mapping file not found: {path}")
        return {}
    try:
        with open(path, "r", encoding="utfâ€‘8") as fh:
            raw = json.load(fh)
        # Normalise to str keys for safety
        return {str(k): v for k, v in raw.items()}
    except Exception as exc:  # noqa: BLE001
        print(f"âŒ Failed to load category map: {exc}")
        return {}

def save_giant_progress(progress):
    """Save current scraping progress"""
    serialisable = progress.copy()
    serialisable['seen_upcs'] = list(serialisable['seen_upcs'])  # Convert set to list for JSON serialization
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump(serialisable, f, indent=2)

def append_to_giant_dump(items):
    """Append items to dump file"""
    if items:
        df = pd.DataFrame(items)
        if os.path.exists(DUMP_FILE):
            df.to_csv(DUMP_FILE, mode='a', header=False, index=False)
        else:
            df.to_csv(DUMP_FILE, index=False)

async def _fetch_json_via_browser(page, url: str) -> Dict[str, Any]:  # type: ignore[override]
    """Wrap the JS *fetch* call so we can reuse it everywhere."""

    escaped = url.replace("'", "\\'")
    js = f"""
        (async () => {{
            try {{
                const res = await fetch('{escaped}', {{
                    method: 'GET',
                    credentials: 'include',
                    headers: {{
                        'accept': 'application/json, text/plain, */*',
                        'user-agent': navigator.userAgent,
                        'referer': location.href
                    }}
                }});

                if (!res.ok) return {{ error: true, status: res.status, message: 'HTTP ' + res.status }};
                const ct = res.headers.get('content-type') || '';
                if (!ct.includes('application/json')) {{
                    const body = await res.text();
                    return {{ error: true, status: res.status, message: 'Nonâ€‘JSON', body: body.slice(0,120) }};
                }}
                const data = await res.json();
                return {{ error: false, status: res.status, data }};
            }} catch (err) {{
                return {{ error: true, status: 0, message: err.message }};
            }}
        }})();
    """
    return await page.evaluate(js)

async def _fetch_with_retry(page, url, max_retries=2, context_recreator=None):
    """Fetch JSON with automatic throttling recovery."""
    for attempt in range(max_retries):
        try:
            res = await _fetch_json_via_browser(page, url)
            status = res.get('status')
            
            # Check for throttling statuses
            if status in [403, 429, 503, 520, 521, 522, 523, 524]:
                if attempt == max_retries - 1:
                    # Final attempt failed - raise exception to trigger context recreation
                    raise Exception(f"Max retries exceeded. Final status: {status}")
                
                print(f"   ğŸ”„ Throttled (status {status}), waiting before retry... (attempt {attempt + 1}/{max_retries})")
                
                # Exponential backoff
                wait_time = (2 ** attempt) * 10  # 10s, 20s, 40s
                print(f"   â³ Waiting {wait_time}s before retry...")
                await asyncio.sleep(wait_time)
                continue
                
            return res
            
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            print(f"   âš ï¸  Request failed: {e}, retrying... (attempt {attempt + 1}/{max_retries})")
            await asyncio.sleep(5)
    
    raise Exception("All retry attempts failed")

async def scrape_giant_category_with_browser(
    page,
    category_id: str | int,
    category_name: str,
    base_params: Dict[str, str],
    store_id: str,
    max_items: int = 10_000,
) -> List[Dict[str, Any]]:
    """Scrape a Giant Foods *main* category, with automatic fallback to subâ€‘cats."""

    category_id = str(category_id)
    category_map = load_category_map()

    progress = load_giant_progress()
    seen_upcs: set[str] = progress.get("seen_upcs", set())
    overall_total: int = progress.get("overall_total", 0)
    category_total: int = progress.get("category_totals", {}).get(category_id, 0)

    api_total = None
    items: List[Dict[str, Any]] = []

    print(f"ğŸ Scraping {category_name} (ID {category_id}) â€¦")

    start = 0
    scroll_limit_hit = progress.get("scroll_limit_reached", {}).get(category_id, False)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1ï¸âƒ£  Parentâ€‘category pagination loop
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    while (not scroll_limit_hit) and len(items) < max_items:
        params = base_params | {"catTreeId": category_id, "start": str(start), "rows": "40"}
        url = f"https://giantfood.com/api/v6.0/products/{store_id}/{FACILITY_ID}?" + urlencode(params)

        print(f"   ğŸ“„ Fetching slice offset={start}")
        
        try:
            res = await _fetch_with_retry(page, url)
        except Exception as e:
            # If throttling persists, raise to trigger context recreation
            if "403" in str(e) or "Throttled" in str(e) or "429" in str(e) or "409" in str(e):
                raise Exception(f"Persistent throttling: {e}")
            print(f"   âš ï¸  Parent call failed after retries: {e}")
            scroll_limit_hit = True
            progress.setdefault("scroll_limit_reached", {})[category_id] = True
            save_giant_progress(progress)
            break

        if res.get("error"):
            status = res.get("status")
            msg = res.get("message")
            print(f"   âš ï¸  Parent call failed ({status}): {msg}")
            scroll_limit_hit = True
            progress.setdefault("scroll_limit_reached", {})[category_id] = True
            save_giant_progress(progress)
            break

        payload = res["data"].get("response", {})
        products = payload.get("products", [])
        pagination = payload.get("pagination", {})

        # total SKUs reported
        if api_total is None:
            api_total = pagination.get("total", 0)
            progress["all_in_category"] = api_total
            save_giant_progress(progress)

        # empty result â†’ hidden cap triggered
        if not products:
            print("   ğŸ“­ No items returned â€“ hidden cap reached, switching to subâ€‘catsâ€¦")
            scroll_limit_hit = True
            progress.setdefault("scroll_limit_reached", {})[category_id] = True
            save_giant_progress(progress)
            break

        new_count = _ingest_products(
            products,
            category_name,
            category_id,
            seen_upcs,
            items,
            progress,
        )
        category_total += new_count
        overall_total += new_count
        progress["overall_total"] = overall_total
        progress.setdefault("category_totals", {})[category_id] = category_total
        save_giant_progress(progress)

        pct_cat = (category_total / api_total * 100) if api_total else 0
        print(
            f"   âœ… +{new_count} (now {category_total}/{api_total} | {pct_cat:.2f}% of category, "
            f"{overall_total} overall)"
        )

        start += 40 
        if start >= pagination.get("total", 0):
            print("   ğŸ“‘ Pagination complete for parent category â†’ done with parent loop")
            break

        await asyncio.sleep(random.uniform(1.5, 3.5))

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2ï¸âƒ£  Subâ€‘category pass via mapping file
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    if str(category_id) in category_map and len(items) < max_items:
        subcats: List[Dict[str, Any]] = category_map[str(category_id)].get("subcategories", [])
        for sc in subcats:
            print(f"ğŸ§© Beginning subâ€‘category scrape for {sc.get('name')} â€¦")
            if len(items) >= max_items:
                break
            try:
                new_items = await _scrape_single_subcat(
                    page,
                    sc,
                    base_params,
                    store_id,
                    category_name,
                    seen_upcs,
                    max_items - len(items),
                )
                items.extend(new_items)
                category_total += len(new_items)
                overall_total += len(new_items)

                progress["overall_total"] = overall_total
                progress.setdefault("category_totals", {})[category_id] = category_total
                save_giant_progress(progress)
            except Exception as exc:
                # If it's a throttling error, re-raise to trigger context recreation
                if "403" in str(exc) or "Throttled" in str(exc) or "429" in str(exc):
                    raise exc
                print(f"   âš ï¸  Subcat '{sc.get('name')}' failed: {exc}")
                # Continue with next subcategory instead of raising
                continue
            await asyncio.sleep(random.uniform(3, 6))
            
    print(f"ğŸ Finished {category_name}: {category_total} items gathered (session +{len(items)})")
    return items

def _ingest_products(
    products: List[Dict[str, Any]],
    category_name: str,
    category_id: str | int,
    seen_upcs: set[str],
    bucket: List[Dict[str, Any]],
    progress: Dict[str, Any],
    *,
    verbose: bool = False
) -> int:
    """Push unique items into *bucket*, return # added."""
    added = 0
    for p in products:
        upc = p.get("upc")
        if not upc or upc in seen_upcs:
            if verbose:
                print(f"â­ï¸  Skipping duplicate or missing UPC: {upc}")
            continue
        seen_upcs.add(upc)
        item = {
            "upc": upc,
            "name": p.get("name"),
            "price": p.get("price"),
            "size": p.get("size"),
            "brand": p.get("brand"),
            "description": p.get("description"),
            "category_name": category_name,
            "category_id": category_id,
            "scraped_timestamp": datetime.now().isoformat(),
        }
        bucket.append(item)
        append_to_giant_dump([item])
        added += 1

    progress["seen_upcs"] = seen_upcs
    return added

async def _scrape_single_subcat(
    page,
    subcat: Dict[str, Any],
    base_params: Dict[str, str],
    store_id: str,
    parent_category_name: str,
    seen_upcs: set[str],
    max_items_remaining: int,
) -> List[Dict[str, Any]]:
    sc_name = subcat.get("name", "(unknown)")

    expected_total = None
    sc_total = 0
    start = 0

    params = base_params | {"catTreeId": subcat.get("cat_tree_id"), "start": str(start), "rows": "40"}
    collected: List[Dict[str, Any]] = []
    
    while len(collected) < max_items_remaining:
        print(f"Starting offset={start} â€¦")
        url = f"https://giantfood.com/api/v6.0/products/{store_id}/{FACILITY_ID}?" + urlencode(params)
        
        try:
            res = await _fetch_with_retry(page, url)
        except Exception as e:
            # If throttling persists, re-raise to trigger context recreation
            if "403" in str(e) or "Throttled" in str(e) or "429" in str(e):
                raise e
            print(f"   âš ï¸  {sc_name}: Failed after retries: {e}")
            break

        if res.get("error"):
            print(f"   âš ï¸  {sc_name}: HTTP {res.get('status')} â€“ aborting sub-cat")
            break

        resp = res["data"].get("response", {})
        products = resp.get("products", [])
        pag = resp.get("pagination", {})
        
        if expected_total is None:
            expected_total = pag.get("total")

        if not products:
            break

        # Fix: Load current progress and save after each batch
        progress = load_giant_progress()
        added = _ingest_products(
            products,
            f"{parent_category_name} > {sc_name}",
            subcat.get("cat_tree_id", "filter"),
            seen_upcs,
            collected,
            progress,
            verbose=False,
        )
        save_giant_progress(progress)
        
        if added == 0:
            # NOTE: FOUND THE LINE!!!!!
            break

        sc_total += added
        start += added

        if expected_total:
            pct = sc_total / expected_total * 100
            print(f"      âœ… +{added} (now {sc_total}/{expected_total} | {pct:.2f}% of sub-category)")
        else:
            print(f"      âœ… +{added} (now {sc_total}/?? items)") 
        await asyncio.sleep(random.uniform(3, 5))

    # final summary
    if expected_total:
        cov = sc_total / expected_total * 100
        print(f"   â€¢ {sc_name}: {sc_total}/{expected_total} items ({cov:.2f}% of sub-category)")
    else:
        print(f"   â€¢ {sc_name}: {sc_total} items (total unknown)")
        
    return collected

async def scrape_with_browser_context():
    """Scrape using browser context throughout"""
    
    async def page_action(page):
        # First capture config
        captured_request = None
        
        def handle_request(request):
            nonlocal captured_request
            if ("/api/v6.0/products/" in request.url and 
                f"/{FACILITY_ID}" in request.url and
                "catTreeId=" in request.url):
                captured_request = request
                print(f"âœ… Captured Giant API request!")
        
        page.on("request", handle_request)
        
        # Set up location
        print("ğŸŒ Setting up Giant Foods location...")
        await page.goto("https://giantfood.com", timeout=30000)
        await page.wait_for_selector('button.robot-shopping-mode-location', timeout=30000)
        await page.click('button.robot-shopping-mode-location', timeout=5000)
        await page.fill("input[name='zipCode']", ZIP_CODE)  
        await page.click('#search-location', timeout=5000)
        await page.wait_for_load_state("networkidle", timeout=17000)
        await page.wait_for_timeout(1000)
        address = page.locator("li", has_text=STORE_ADDRESS)
        await address.locator("button").click(timeout=7000)
        await page.wait_for_timeout(5000)
        
        # Navigate to category to trigger API call
        await page.goto("https://giantfood.com/browse-aisles/categories/1/categories/2098-produce", timeout=30000)
        await page.wait_for_timeout(3000)
        
        # Try to trigger API call
        try:
            load_more_button = page.locator('button:has-text("Load More")')
            if await load_more_button.count() > 0:
                await load_more_button.click()
                await page.wait_for_timeout(2000)
        except:
            pass
        
        # Wait for captured request
        max_wait = 90
        waited = 0
        while not captured_request and waited < max_wait:
            await asyncio.sleep(0.5)
            waited += 0.5
        
        if not captured_request:
            raise Exception("Timeout waiting for Giant API request")
        
        print("ğŸ‰ Processing captured Giant request...")
        
        # Extract parameters from captured request
        parsed_url = urlparse(captured_request.url)
        captured_params = parse_qs(parsed_url.query)

        extracted_params = {}
        for key, value_list in captured_params.items():
            extracted_params[key] = value_list[0] if value_list else ''

        base_params = {
            'sort': extracted_params.get('sort', 'itemsPurchased desc, name asc'),
            'filter': extracted_params.get('filter', ''),
            'flags': extracted_params.get('flags', 'true'),
            'substitute': extracted_params.get('substitute', 'false'),
            'nutrition': extracted_params.get('nutrition', 'false'),
            'extendedInfo': extracted_params.get('extendedInfo', 'false'),
            'facetExcludeFilter': extracted_params.get('facetExcludeFilter', 'true'),
            'dtmCookieId': extracted_params.get('dtmCookieId', ''),
            'adSessionId': extracted_params.get('adSessionId', ''),
            'platform': extracted_params.get('platform', 'desktop'),
            'includeSponsoredProducts': extracted_params.get('includeSponsoredProducts', 'true'),
            'facet': extracted_params.get('facet', 'specials,categories,brands,nutrition,sustainability,newArrivals,privateLabel')
        }
        
        # Extract store ID
        import re
        url_pattern = r'/api/v6\.0/products/(\d+)/' + FACILITY_ID
        match = re.search(url_pattern, captured_request.url)
        store_id = match.group(1) if match else "330070638"  # fallback
        
        print(f"ğŸª Using store ID: {store_id}")
        print(f"ğŸ”— Session ID: {base_params['adSessionId']}")
        
        # Now scrape all categories using the same browser context
        progress = load_giant_progress()
        await page.wait_for_load_state("networkidle", timeout = 30000)
        
        for category_id, category_name in MAIN_CATEGORIES.items():
            category_key = f"{category_id}::{category_name}"
            
            if category_key in progress['completed_categories']:
                print(f"â­ï¸ Skipping completed: {category_name}")
                continue
            
            print(f"\nğŸ—‚ï¸ Starting: {category_name}")
            
            try:
                max_items = 10000
                items = await scrape_giant_category_with_browser(
                    page, category_id, category_name, base_params, store_id, max_items
                )
    
                if items:
                    print(f"âœ… Completed {category_name}: {len(items)} items")
                    progress = load_giant_progress()  # Reload to get updated counts
                    progress['completed_categories'].append(category_key)
                else:
                    print(f"âš ï¸ No items found for {category_name}")
                    progress = load_giant_progress()
                    progress['failed_categories'].append({
                        'category': category_key,
                        'reason': 'No items found',
                        'timestamp': datetime.now().isoformat()
                    })
                
                save_giant_progress(progress)
                
            except Exception as e:
                if "403" in str(e) or "Throttled" in str(e) or "429" in str(e):
                    print(f"ğŸš« Got throttled on {category_name}! Will restart browser context.")
                    # Return special signal to restart context
                    return "THROTTLED"
                else:
                    print(f"âš ï¸ Failed to scrape {category_name}: {e}")
                    progress = load_giant_progress()
                    progress['failed_categories'].append({
                        'category': category_key,
                        'reason': str(e),
                        'timestamp': datetime.now().isoformat()
                    })
                    save_giant_progress(progress)
            
            # Delay between categories
            await asyncio.sleep(random.uniform(10, 20))
        
        print("ğŸ‰ All categories completed!")
        return "COMPLETED"
    
    # Run with browser context
    result = await StealthyFetcher.async_fetch(
        url="https://giantfood.com",
        headless=False,
        network_idle=True,
        block_images=False,
        disable_resources=False,
        page_action=page_action
    )
    
    return result

async def scrape_giant_with_throttle_recovery(max_retries=999):
    """Main Giant scraping function using browser context requests"""
    
    # Load progress
    progress = load_giant_progress()
    print(f"ğŸ“ˆ Loaded progress: {len(progress['completed_categories'])} completed, {len(progress['failed_categories'])} failed, overall total: {progress['overall_total']}")

    total_categories = len(MAIN_CATEGORIES)
    completed_count = len(progress['completed_categories'])

    print(f"ğŸ¯ Total categories: {total_categories}")
    print(f"ğŸ¯ Starting from: {completed_count}/{total_categories}")

    retry_count = 0
    
    while retry_count <= max_retries:
        try:
            print(f"\nğŸ”„ Attempt {retry_count + 1}/{max_retries + 1} - Starting browser session...")
            
            # Get fresh config and maintain browser context for scraping
            config_result = await scrape_with_browser_context()
            
            if config_result == "COMPLETED":
                print("ğŸ‰ Scraping completed successfully!")
                return True
            elif config_result == "THROTTLED":
                print("âš ï¸ Got throttled, will retry with new browser context...")
            else:
                print("âš ï¸ Scraping session ended unexpectedly, will retry...")
                
        except Exception as e:
            print(f"âŒ Unexpected error in attempt {retry_count + 1}: {e}")
        
        retry_count += 1
        
        if retry_count <= max_retries:
            # Check if we've completed all categories
            progress = load_giant_progress()
            completed_count = len(progress['completed_categories'])
            
            if completed_count >= total_categories:
                print("ğŸ‰ All categories completed!")
                return True
                
            wait_time = 3
            print(f"â³ Waiting {wait_time} seconds before recreating context...")
            await asyncio.sleep(wait_time)
        else:
            print(f"\nğŸ›‘ Maximum retries ({max_retries}) reached. Stopping.")
            break
    
    # Final status
    progress = load_giant_progress()
    print(f"\nğŸ“Š Final Status:")
    print(f"âœ… Completed: {len(progress['completed_categories'])}/{total_categories} categories")
    print(f"âŒ Failed: {len(progress['failed_categories'])} categories")
    
    if os.path.exists(DUMP_FILE):
        try:
            df = pd.read_csv(DUMP_FILE)
            print(f"ğŸ“ Total items scraped: {len(df)}")
            print(f"ğŸ“ Data saved to: {DUMP_FILE}")
        except:
            print(f"ğŸ“ Data saved to: {DUMP_FILE}")
        print(f"ğŸ”„ Run script again to retry failed categories")
    
    return False

def match_giant_upcs_and_create_comparison():
    """Match UPCs between input dataset and Giant scraped data, create comparison file"""
    
    if not os.path.exists(SOURCE_DATA):
        print(f"âŒ Input file not found: {SOURCE_DATA}")
        return False
    
    try:
        # Load input dataset
        print(f"ğŸ“– Loading input dataset: {SOURCE_DATA}")
        input_df = pd.read_excel(SOURCE_DATA)
        print(f"ğŸ“Š Input dataset: {len(input_df)} rows")
        
        # Check if UPC column exists
        if 'UPC' not in input_df.columns:
            print(f"âŒ 'UPC' column not found in input dataset")
            print(f"Available columns: {list(input_df.columns)}")
            return False
        
        # Load scraped data
        if not os.path.exists(DUMP_FILE):
            print(f"âŒ Scraped data file not found: {DUMP_FILE}")
            return False
        
        print(f"ğŸ“– Loading Giant scraped dataset: {DUMP_FILE}")
        try:
            scraped_df = pd.read_csv(DUMP_FILE)
        except pd.errors.ParserError as e:
            print(f"âš ï¸ CSV parsing error: {e}")
            print("ğŸ”§ Trying to fix CSV parsing issues...")
            
            try:
                scraped_df = pd.read_csv(DUMP_FILE, 
                                    on_bad_lines='skip',
                                    quoting=1,
                                    escapechar='\\')
            except Exception as final_error:
                print(f"âŒ Could not load CSV file: {final_error}")
                return False

        print(f"ğŸ“Š Giant scraped dataset: {len(scraped_df)} rows")
        
        # Normalize UPCs
        input_df['normalized_upc'] = input_df['UPC'].apply(normalize_upc_input)
        scraped_df['normalized_upc'] = scraped_df['upc'].apply(normalize_upc_scraped)

        input_upcs = set(input_df['normalized_upc'].dropna())
        scraped_upcs = set(scraped_df['normalized_upc'].dropna())
        
        print(f"ğŸ” Input UPCs: {len(input_upcs)}")
        print(f"ğŸ” Giant UPCs: {len(scraped_upcs)}")
        
        # Find matches
        matched_upcs = input_upcs.intersection(scraped_upcs)
        print(f"âœ… Matched UPCs: {len(matched_upcs)}")
        
        if not matched_upcs:
            print("âŒ No UPC matches found between datasets")
            return False
        
        # Create comparison dataset
        comparison_data = []
        
        for upc in matched_upcs:
            # Get original data
            original_rows = input_df[input_df['normalized_upc'].astype(str) == upc]
            
            # Get scraped data
            scraped_rows = scraped_df[scraped_df['normalized_upc'].astype(str) == upc]
            
            for _, original_row in original_rows.iterrows():
                for _, scraped_row in scraped_rows.iterrows():
                    # Combine data
                    combined_row = {
                        # Original data (keep all columns)
                        **original_row.to_dict(),
                        # Add Giant data with prefixes
                        'giant_upc': scraped_row.get('upc'),
                        'giant_price': scraped_row.get('price'),
                        'giant_category': scraped_row.get('category_name'),
                        'giant_brand': scraped_row.get('brand'),
                        'giant_name': scraped_row.get('name'),
                        'giant_size': scraped_row.get('size'),
                        'giant_description': scraped_row.get('description'),
                        'giant_scraped_at': scraped_row.get('scraped_timestamp')
                    }
                    comparison_data.append(combined_row)
        
        # Create comparison DataFrame
        comparison_df = pd.DataFrame(comparison_data)
        
        # Save to Excel with multiple sheets
        with pd.ExcelWriter(OUTPUT_DATA, engine='openpyxl') as writer:
            # All results
            comparison_df.to_excel(writer, sheet_name='All_Comparisons', index=False)
            
            # Matches with prices for comparison
            price_comparison = comparison_df.dropna(subset=['giant_price'])
            if not price_comparison.empty:
                price_comparison.to_excel(writer, sheet_name='Price_Comparisons', index=False)
        
        print(f"\nğŸ‰ Giant UPC matching completed!")
        print(f"ğŸ“Š Matched products: {len(comparison_df)}")
        print(f"ğŸ“ Comparison file saved to: {OUTPUT_DATA}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error during Giant UPC matching: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """Main function with options for scraping or UPC matching"""
    
    print("ğŸª Giant Foods Product Scraper & Price Comparator")
    print("=" * 50)
    print("1. Scrape products from Giant Foods")
    print("2. Match UPCs and create price comparison")
    print("3. Both (scrape then compare)")
    
    while True:
        choice = input("\nEnter your choice (1, 2, or 3): ").strip()
        
        if choice == "1":
            # Scraping only
            await run_giant_scraping()
            break
        elif choice == "2":
            # UPC matching only
            success = match_giant_upcs_and_create_comparison()
            if success:
                print("âœ… Giant UPC matching completed successfully!")
            else:
                print("âŒ Giant UPC matching failed")
            break
        elif choice == "3":
            # Both scraping and matching
            await run_giant_scraping()
            print("\n" + "="*50)
            print("ğŸ”„ Now starting UPC matching...")
            success = match_giant_upcs_and_create_comparison()
            if success:
                print("âœ… Both scraping and UPC matching completed!")
            else:
                print("âš ï¸ Scraping completed but UPC matching failed")
            break
        else:
            print("âŒ Invalid choice. Please enter 1, 2, or 3.")

async def run_giant_scraping():
    """Run the Giant scraping functionality"""
    # Start scraping with automatic throttle recovery
    success = await scrape_giant_with_throttle_recovery(max_retries=999)
    
    if success:
        print("ğŸ‰ Giant scraping completed successfully!")
    else:
        print("âš ï¸ Giant scraping completed with some limitations due to throttling")

# Utility functions
def show_giant_progress():
    """Show current Giant scraping progress"""
    progress = load_giant_progress()
    completed = len(progress['completed_categories'])
    total = len(MAIN_CATEGORIES)
    
    print(f"\nğŸ“Š Giant Catalog Scraping Progress:")
    print(f"   Categories: {completed}/{total} ({completed/total*100:.1f}%)")
    
    if os.path.exists(DUMP_FILE):
        try:
            df = pd.read_csv(DUMP_FILE)
            print(f"   Products scraped: {len(df)}")
        except:
            print("   Products scraped: (file exists but couldn't read)")
    
    if completed < total:
        remaining = [name for cat_id, name in MAIN_CATEGORIES.items() 
                    if f"{cat_id}::{name}" not in progress['completed_categories']]
        print(f"   Remaining: {', '.join(remaining[:5])}{'...' if len(remaining) > 5 else ''}")

def reset_giant_progress():
    """Reset Giant progress and dump file"""
    files_to_remove = [CHECKPOINT_FILE, DUMP_FILE]
    
    for file_path in files_to_remove:
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"ğŸ—‘ï¸ Removed {file_path}")
        else:
            print(f"â„¹ï¸ {file_path} not found")
    
    print("ğŸ”„ Giant progress reset complete")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        if sys.argv[1] == "status":
            show_giant_progress()
        elif sys.argv[1] == "reset":
            reset_giant_progress()
        else:
            print("Usage: python giant_catalog_scraper.py [status|reset]")
            print("  status - Show current scraping progress")
            print("  reset  - Reset progress and start over")
            print("  (no args) - Run scraping/comparison session")
    else:
        asyncio.run(main())