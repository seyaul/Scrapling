import asyncio
import json
import os
from urllib.parse import urlencode, urlparse, parse_qs
import pandas as pd
import httpx
from scrapling.fetchers import StealthyFetcher

# Constants
SAF_WELCOME = "https://www.safeway.com"
CONFIG_FILE = 'scraplingAdaptationHana/.safeway_config.json'
CATEGORIES_FILE = 'safeway_categories.json'
OUTPUT_CSV = 'bread_bakery_scraped.csv'

async def gather_scrape_config():
    """Capture session data by monitoring API requests"""
    async def page_action(page):
        captured_request = None
        
        def handle_request(request):
            nonlocal captured_request
            if "/xapi/v1/aisles/products" in request.url:
                captured_request = request
                print(f"âœ… Captured API request!")
        
        page.on("request", handle_request)
        
        await page.goto(SAF_WELCOME, timeout=20000)
        print("ğŸŒ Please manually:")
        print("   1. Set your store/ZIP code")
        print("   2. Navigate to ANY bread & bakery subcategory")
        print("   3. The script will automatically detect the API call")
        
        # Wait for the request with timeout
        max_wait = 120  # 2 minutes
        waited = 0
        while not captured_request and waited < max_wait:
            await asyncio.sleep(0.5)
            waited += 0.5
        
        if not captured_request:
            print("âŒ Timeout waiting for API request")
            return None
        
        try:
            print("ğŸ‰ Processing captured request...")
            
            # Extract query params
            parsed = urlparse(captured_request.url)
            raw_params = parse_qs(parsed.query)
            query_params = {k: v[0] for k, v in raw_params.items()}
            
            # Extract cookies and headers
            cookies = await page.context.cookies()
            cookie_jar = {c['name']: c['value'] for c in cookies if 'safeway.com' in c['domain']}
            
            # Clean headers - filter out None values
            raw_headers = {
                'accept': 'application/json, text/plain, */*',
                'accept-language': 'en',
                'user-agent': captured_request.headers.get('user-agent'),
                'referer': captured_request.headers.get('referer'),
                'sec-ch-ua': '"Chromium";v="136", "Google Chrome";v="136", "Not.A/Brand";v="99"',
                'sec-ch-ua-mobile': '?0',
                'sec-ch-ua-platform': '"macOS"',
                'sec-fetch-dest': 'empty',
                'sec-fetch-mode': 'cors',
                'sec-fetch-site': 'same-origin',
                'priority': 'u=1, i'
            }
            
            # Filter out None values
            headers = {k: v for k, v in raw_headers.items() if v is not None}
            
            # Save config
            config = {
                'query_params': query_params,
                'headers': headers,
                'cookies': cookie_jar
            }
            
            os.makedirs(os.path.dirname(CONFIG_FILE), exist_ok=True)
            with open(CONFIG_FILE, 'w') as f:
                json.dump(config, f, indent=2)
            
            print(f"âœ… Saved scrape config to {CONFIG_FILE}")
            return page
            
        except Exception as e:
            print(f"âŒ Error processing request: {e}")
            return None
    
    await StealthyFetcher.async_fetch(
        url=SAF_WELCOME,
        headless=False,
        network_idle=True,
        block_images=False,
        disable_resources=False,
        page_action=page_action
    )

async def scrape_single_category(client, base_params, category_id, category_name, max_items=300):
    """Scrape a single category with pagination handling"""
    seen_upcs = set()
    items = []
    start = 0
    rows = 20  # Items per page
    next_token = None
    
    print(f"ğŸ Scraping {category_name} (ID: {category_id})...")
    
    while len(items) < max_items:
        # Build parameters for this request
        params = base_params.copy()
        params.update({
            'category-id': category_id,
            'category-name': category_name,
            'start': str(start),
            'rows': str(rows)
        })
        
        if next_token:
            params['nextPageToken'] = next_token
        
        # Build URL
        url = 'https://www.safeway.com/abs/pub/xapi/v1/aisles/products?' + urlencode(params)
        
        print(f"ğŸ“„ Fetching page starting at item {start}...")
        
        try:
            print(f"ğŸ” Request URL: {url}")
            resp = await client.get(url)
            resp.raise_for_status()
            
            # Debug the response
            response_text = resp.text
            print(f"ğŸ“Š Response status: {resp.status_code}")
            print(f"ğŸ“Š Response length: {len(response_text)}")
            
            try:
                json_data = resp.json()
                print(f"ğŸ“Š JSON keys: {list(json_data.keys())}")
                
                data = json_data.get('response', {})
                print(f"ğŸ” DEBUG - response keys: {list(data.keys())}")
                print(f"ğŸ” DEBUG - numFound: {data.get('numFound')}")
                print(f"ğŸ” DEBUG - start: {data.get('start')}")
                print(f"ğŸ” DEBUG - nextPageToken: {data.get('nextPageToken')}")
                print(f"ğŸ” DEBUG - miscInfo: {data.get('miscInfo', {}).keys()}")
                
                docs = data.get('docs', [])
                print(f"ğŸ“Š Found {len(docs)} docs")
                
                # If no docs, let's see what we got
                if not docs:
                    print(f"ğŸ“Š Full response: {json_data}")
                    print("ğŸ“­ No more items found")
                    break
                    
            except json.JSONDecodeError as e:
                print(f"âŒ JSON decode error: {e}")
                print(f"ğŸ“Š Raw response: {response_text[:500]}...")
                break
            
            # Process items from this page
            page_items = 0
            for doc in docs:
                upc = doc.get('upc')
                if upc and upc not in seen_upcs:
                    seen_upcs.add(upc)
                    # Add category context
                    doc['category_name'] = category_name
                    doc['category_id'] = category_id
                    items.append(doc)
                    page_items += 1
                    
                    if len(items) >= max_items:
                        break
            
            print(f"âœ… Added {page_items} new items (total: {len(items)})")
            
            # Get next page token
            next_token = data.get('miscInfo',{}).get('nextPageToken')
            if not next_token or next_token == "":
                print("ğŸ“„ No more pages available")
                break
            
            # Update start for next iteration
            start += rows
            
            # Add delay to avoid rate limiting
            #await asyncio.sleep(2)
            
        except httpx.HTTPError as e:
            print(f"âŒ HTTP Error: {e}")
            if "403" in str(e):
                print("ğŸš« Got blocked (403) - stopping scrape")
                break
            elif "400" in str(e):
                print("âš ï¸ Bad request (400) - check parameters")
                break
        except Exception as e:
            print(f"âŒ Unexpected error: {e}")
            break
    
    return items

async def main():
    """Main function to scrape bread & bakery categories"""
    
    # Always generate fresh config
    if os.path.exists(CONFIG_FILE):
        os.remove(CONFIG_FILE)
        print("ğŸ”„ Removed old config file")
    
    print("ğŸ“¦ Gathering fresh store and session info...")
    await gather_scrape_config()
    
    if not os.path.exists(CONFIG_FILE):
        print("âŒ Config file was not created. Exiting.")
        return
    
    # Load config
    try:
        with open(CONFIG_FILE) as f:
            config = json.load(f)
    except (json.JSONDecodeError, FileNotFoundError):
        print("âŒ Failed to load config. Exiting.")
        return
    
    # Load categories
    try:
        with open(CATEGORIES_FILE) as f:
            categories_map = json.load(f)
    except FileNotFoundError:
        print(f"âŒ Categories file {CATEGORIES_FILE} not found.")
        return
    
    # Setup request parameters
    base_params = config['query_params'].copy()
    headers = config['headers'].copy()
    cookies = config.get('cookies', {})
    
    # Ask for subscription key if not already in headers
    if 'ocp-apim-subscription-key' not in headers:
        sub_key = input('ğŸ”‘ Enter your ocp-apim-subscription-key: ').strip()
        headers['ocp-apim-subscription-key'] = sub_key
    
    # Get bread & bakery categories (excluding "All Bread & Bakery")
    bread_categories = categories_map.get("Bread & Bakery", [])
    target_categories = [cat for cat in bread_categories if "All Bread" not in cat['display_name']]
    
    print(f"ğŸ¯ Found {len(target_categories)} bread & bakery subcategories to scrape")
    
    all_items = []
    
    async with httpx.AsyncClient(headers=headers, cookies=cookies, timeout=30) as client:
        # Test with just the first category for debugging
        category = target_categories[0]
        print(f"\nğŸ§ª DEBUG MODE - Testing single category: {category['display_name']}")
        print(f"ğŸ§ª Category href: {category['href']}")
        
        # Extract category info from href
        parsed = urlparse(category['href'])
        query_params = parse_qs(parsed.query)
        
        print(f"ğŸ§ª Parsed URL: {parsed}")
        print(f"ğŸ§ª Query params from URL: {query_params}")
        
        # Build category name from URL path
        path_parts = parsed.path.strip('/').split('/')
        print(f"ğŸ§ª Path parts: {path_parts}")
        
        print
        category_id = base_params.get('category-id', '1_2_10')  # Use captured ID as fallback
        category_name = f"Bread & Bakery > {category['display_name']}"
        
        print(f"ğŸ§ª Generated category_name: {category_name}")
        print(f"ğŸ§ª Generated category_id: {category_id}")
        print(f"ğŸ§ª Base params: {base_params}")
        
        try:
            items = await scrape_single_category(
                client, base_params, category_id, category_name, max_items=300  # Reduced for testing
            )
            all_items.extend(items)
            
            print(f"âœ… Completed {category_name}: {len(items)} items")
            
        except Exception as e:
            print(f"âš ï¸ Failed to scrape {category_name}: {e}")
            import traceback
            traceback.print_exc()
    
    # Save results
    if all_items:
        df = pd.DataFrame(all_items)
        df.to_csv(OUTPUT_CSV, index=False)
        print(f"\nğŸ‰ Success! Scraped {len(all_items)} total items")
        print(f"ğŸ“ Saved to: {OUTPUT_CSV}")
        print(f"ğŸ·ï¸ Unique categories: {df['category_name'].nunique()}")
    else:
        print("âŒ No items were scraped")

if __name__ == '__main__':
    asyncio.run(main())